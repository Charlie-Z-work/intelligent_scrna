import numpy as np
import pandas as pd
import h5py
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import VarianceThreshold
import warnings
warnings.filterwarnings('ignore')

print("ğŸ”§ ZINBå‹å¥½çš„é¢„å¤„ç†...")

# 1. è¯»å–åŸå§‹è®¡æ•°æ•°æ®
X_raw = np.loadtxt("data/mtab.txt", dtype=float)
labels = np.loadtxt("data/mtab_label.txt", dtype=int)

print(f"åŸå§‹è®¡æ•°æ•°æ®: {X_raw.shape}")

# 2. åŸºæœ¬è¿‡æ»¤ï¼ˆä¿æŒè®¡æ•°æ€§è´¨ï¼‰
# è¿‡æ»¤è¡¨è¾¾é‡å¾ˆä½çš„åŸºå› 
gene_counts = np.sum(X_raw > 0, axis=0)  # æ¯ä¸ªåŸºå› åœ¨å¤šå°‘ç»†èƒä¸­è¡¨è¾¾
keep_genes = gene_counts >= 3  # è‡³å°‘åœ¨3ä¸ªç»†èƒä¸­è¡¨è¾¾
X_filtered = X_raw[:, keep_genes]

print(f"åŸºå› è¿‡æ»¤å: {X_filtered.shape}")

# 3. é€‰æ‹©é«˜å˜å¼‚åŸºå› ï¼ˆåŸºäºè®¡æ•°æ•°æ®ï¼‰
if X_filtered.shape[1] > 5000:
    gene_var = np.var(X_filtered, axis=0)
    top_indices = np.argsort(gene_var)[-5000:]
    X_final = X_filtered[:, top_indices]
else:
    X_final = X_filtered

print(f"æœ€ç»ˆè®¡æ•°æ•°æ®: {X_final.shape}")

# 4. ç¡®ä¿æ•°æ®ä¸ºéè´Ÿ
X_final = np.maximum(X_final, 0)

# 5. è®¡ç®—æ­£ç¡®çš„size factors
library_sizes = np.sum(X_final, axis=1)
median_library = np.median(library_sizes)
size_factors = library_sizes / median_library

print(f"Size factorsèŒƒå›´: [{size_factors.min():.3f}, {size_factors.max():.3f}]")

# 6. ç¼–ç æ ‡ç­¾
le = LabelEncoder()
labels_encoded = le.fit_transform(labels)

# 7. ä¿å­˜åŸå§‹è®¡æ•°æ•°æ®ï¼ˆç”¨äºZINBï¼‰
np.savetxt("data/mtab_counts_raw.txt", X_final, delimiter='\t', fmt='%.0f')
np.savetxt("data/mtab_processed_label.txt", labels_encoded, fmt='%d')

# 8. åˆ›å»ºæ ‡å‡†åŒ–ç‰ˆæœ¬ï¼ˆç”¨äºå…¶ä»–æ¨¡å—ï¼‰
X_normalized = X_final / library_sizes[:, np.newaxis] * median_library
X_log = np.log1p(X_normalized)
# è½»åº¦æ ‡å‡†åŒ–
X_scaled = (X_log - np.mean(X_log, axis=0)) / (np.std(X_log, axis=0) + 1e-8)
X_scaled = np.clip(X_scaled, -10, 10)  # é™åˆ¶èŒƒå›´

np.savetxt("data/mtab_processed.txt", X_scaled, delimiter='\t')

# 9. åˆ›å»ºH5æ–‡ä»¶ï¼ˆåŒæ—¶åŒ…å«åŸå§‹å’Œå¤„ç†åçš„æ•°æ®ï¼‰
with h5py.File("data/mtab.h5", "w") as f:
    f.create_dataset("X", data=X_scaled)        # æ ‡å‡†åŒ–æ•°æ®ï¼ˆç”¨äºGNNç­‰ï¼‰
    f.create_dataset("X_raw", data=X_final)     # åŸå§‹è®¡æ•°ï¼ˆç”¨äºZINBï¼‰
    f.create_dataset("Y", data=labels_encoded)
    f.create_dataset("size_factors", data=size_factors)

print("âœ… ZINBå‹å¥½é¢„å¤„ç†å®Œæˆ!")
print(f"åŸå§‹è®¡æ•°: {X_final.shape}, size factorsæ­£å¸¸: {size_factors.min():.3f}-{size_factors.max():.3f}")
