#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½å•ç»†èƒRNAæµ‹åºåˆ†æç³»ç»Ÿ - ä¸»æ§æ–‡ä»¶
åŸºäºå‡ ä½•ç‰¹å¾å’Œè¾¹ç•Œå¤±è´¥å­¦ä¹ çš„è‡ªé€‚åº”ç®—æ³•é€‰æ‹©æ¡†æ¶

ä½œè€…: [Your Name]
ç‰ˆæœ¬: 1.0.0
æ ¸å¿ƒæ€æƒ³: "è¾¹ç•Œå¤±è´¥ä¸æ˜¯å™ªå£°ï¼Œè€Œæ˜¯ç»“æ„æ¼”åŒ–çš„ä¿¡å·"
"""

import argparse
import time
from pathlib import Path
import pandas as pd
import numpy as np
from typing import Dict, Any, Optional

from core.geometry_analyzer import GeometryAnalyzer
from core.strategy_atlas import StrategyAtlas
from core.iterative_learner import IterativeLearner
from core.failure_analyzer import FailureAnalyzer
from utils.data_loader import DataLoader
from utils.metrics import MetricsCalculator
from utils.visualization import ResultVisualizer
from config import Config

class IntelligentScRNAAtlas:
    """
    æ™ºèƒ½å•ç»†èƒRNAæµ‹åºåˆ†æç³»ç»Ÿ
    
    æ ¸å¿ƒåŠŸèƒ½:
    1. å‡ ä½•ç‰¹å¾åˆ†æ
    2. ç­–ç•¥å›¾è°±åŒ¹é…
    3. å¤±è´¥å­¦ä¹ ä¼˜åŒ–
    4. çŸ¥è¯†åº“æ›´æ–°
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        self.config = Config(config_path)
        
        # æ ¸å¿ƒç»„ä»¶
        self.geometry_analyzer = GeometryAnalyzer(self.config)
        self.strategy_atlas = StrategyAtlas(self.config.atlas_path)
        self.failure_analyzer = FailureAnalyzer(self.config)
        self.iterative_learner = IterativeLearner(
            self.strategy_atlas, 
            self.failure_analyzer,
            self.config
        )
        
        # å·¥å…·ç»„ä»¶
        self.data_loader = DataLoader(self.config)
        self.metrics_calc = MetricsCalculator()
        self.visualizer = ResultVisualizer(self.config)
        
        print(f"ğŸŒŸ æ™ºèƒ½scRNAåˆ†æç³»ç»Ÿå·²å¯åŠ¨")
        print(f"ğŸ’¡ æ ¸å¿ƒæ€æƒ³: {self.config.core_philosophy}")
    
    def analyze_dataset(self, 
                       data_path: str, 
                       labels_path: Optional[str] = None,
                       dataset_name: str = "unknown",
                       max_iterations: int = 5,
                       save_results: bool = True) -> Dict[str, Any]:
        """
        åˆ†æå•ä¸ªæ•°æ®é›†
        
        æµç¨‹:
        1. æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
        2. å‡ ä½•ç‰¹å¾åˆ†æ
        3. ç­–ç•¥å›¾è°±åŒ¹é…
        4. è¿­ä»£å¤±è´¥å­¦ä¹ 
        5. ç»“æœä¿å­˜å’Œå¯è§†åŒ–
        """
        
        print(f"\nğŸ”¬ å¼€å§‹åˆ†ææ•°æ®é›†: {dataset_name}")
        print("=" * 60)
        
        start_time = time.time()
        
        # 1. æ•°æ®åŠ è½½
        print("ğŸ“Š åŠ è½½æ•°æ®...")
        X, y_true = self.data_loader.load_dataset(data_path, labels_path)
        print(f"   æ•°æ®ç»´åº¦: {X.shape}")
        if y_true is not None:
            print(f"   ç±»åˆ«æ•°: {len(np.unique(y_true))}")
        
        # 2. å‡ ä½•ç‰¹å¾åˆ†æ
        print("\nğŸ” åˆ†æå‡ ä½•ç‰¹å¾...")
        geometry_features = self.geometry_analyzer.analyze(X, y_true)
        self._print_geometry_summary(geometry_features)
        
        # 3. ç­–ç•¥å›¾è°±åŒ¹é…
        print("\nğŸ—ºï¸ ç­–ç•¥å›¾è°±åŒ¹é…...")
        initial_strategy = self.strategy_atlas.find_best_match(geometry_features)
        
        # ç¡®ä¿ç­–ç•¥è¯¦ç»†é…ç½®è¢«æ­£ç¡®ä¼ é€’
        if 'strategy_details' in initial_strategy:
            strategy_details = initial_strategy['strategy_details']
            # å°†è¯¦ç»†é…ç½®åˆå¹¶åˆ°åˆå§‹ç­–ç•¥ä¸­
            for key, value in strategy_details.items():
                if key not in initial_strategy:
                    initial_strategy[key] = value
            print(f"   ğŸ“‹ åº”ç”¨ç­–ç•¥é…ç½®: PCA-{initial_strategy.get('pca_components', 'unknown')}ç»´")
        print(f"   æ¨èç­–ç•¥: {initial_strategy['name']}")
        print(f"   åŒ¹é…ç›¸ä¼¼åº¦: {initial_strategy['similarity']:.3f}")
        print(f"   é¢„æœŸæ€§èƒ½: {initial_strategy['expected_nmi']:.3f}")
        
        # 4. è¿­ä»£å¤±è´¥å­¦ä¹ 
        print("\nğŸ¯ è¿­ä»£å­¦ä¹ ä¼˜åŒ–...")
        learning_result = self.iterative_learner.optimize(
            X, y_true, initial_strategy, max_iterations
        )

        # å¢åŠ å®‰å…¨æ£€æŸ¥
        if learning_result is None:
            print("âš ï¸ å­¦ä¹ ç»“æœä¸ºç©ºï¼Œåˆ›å»ºå¤‡ç”¨ç»“æœ")
            learning_result = {
                'trajectory': [],
                'final_performance': {'nmi': 0.0, 'ari': 0.0, 'silhouette': 0.0},
                'total_improvement': 0,
                'iterations_used': 0
            }

        # 5. ç»“æœæ•´ç†
        final_result = {
            'dataset_name': dataset_name,
            'data_shape': X.shape,
            'geometry_features': geometry_features,
            'initial_strategy': initial_strategy,
            'learning_trajectory': learning_result['trajectory'],
            'final_performance': learning_result['final_performance'],
            'total_improvement': learning_result['total_improvement'],
            'iterations_used': learning_result['iterations_used'],
            'processing_time': time.time() - start_time
        }
        
        # 6. ç»“æœå±•ç¤º
        self._print_final_results(final_result)
        
        # 7. ä¿å­˜å’Œå¯è§†åŒ–
        if save_results:
            self._save_results(final_result, dataset_name)
            self.visualizer.plot_learning_trajectory(final_result)
        
        # 8. æ›´æ–°å›¾è°±çŸ¥è¯†åº“
        self._update_atlas_knowledge(final_result)
        
        return final_result
    
    def batch_analyze(self, 
                     datasets_config: str,
                     output_dir: str = "results") -> Dict[str, Any]:
        """
        æ‰¹é‡åˆ†æå¤šä¸ªæ•°æ®é›†
        """
        print(f"\nğŸš€ æ‰¹é‡åˆ†ææ¨¡å¼")
        
        # åŠ è½½æ•°æ®é›†é…ç½®
        datasets = self.data_loader.load_datasets_config(datasets_config)
        
        batch_results = {}
        summary_stats = {
            'total_datasets': len(datasets),
            'successful_analyses': 0,
            'average_improvement': 0,
            'best_strategies': {}
        }
        
        for dataset_name, dataset_info in datasets.items():
            try:
                print(f"\nå¤„ç†æ•°æ®é›† {dataset_name}...")
                result = self.analyze_dataset(
                    dataset_info['data_path'],
                    dataset_info.get('labels_path'),
                    dataset_name,
                    save_results=True
                )
                
                batch_results[dataset_name] = result
                summary_stats['successful_analyses'] += 1
                
            except Exception as e:
                print(f"âŒ æ•°æ®é›† {dataset_name} åˆ†æå¤±è´¥: {e}")
                batch_results[dataset_name] = {'error': str(e)}
        
        # ç”Ÿæˆæ‰¹é‡åˆ†ææŠ¥å‘Š
        self._generate_batch_report(batch_results, summary_stats, output_dir)
        
        return batch_results
    
    def benchmark_against_baselines(self, 
                                  datasets_config: str,
                                  baseline_methods: list = None) -> Dict[str, Any]:
        """
        ä¸åŸºçº¿æ–¹æ³•å¯¹æ¯”æµ‹è¯•
        """
        if baseline_methods is None:
            baseline_methods = ['SIMLR', 'scDSC', 'Seurat', 'Scanpy']
        
        print(f"\nğŸ“Š åŸºå‡†æµ‹è¯•æ¨¡å¼")
        print(f"å¯¹æ¯”æ–¹æ³•: {baseline_methods}")
        
        # å®ç°åŸºçº¿æ–¹æ³•å¯¹æ¯”é€»è¾‘
        benchmark_results = {}
        
        # TODO: å®ç°å…·ä½“çš„åŸºçº¿æ–¹æ³•è°ƒç”¨
        
        return benchmark_results
    
    def _print_geometry_summary(self, features: Dict[str, Any]):
        """æ‰“å°å‡ ä½•ç‰¹å¾æ‘˜è¦"""
        print(f"   æ ·æœ¬æ•°: {features['basic']['n_samples']}")
        print(f"   ç‰¹å¾æ•°: {features['basic']['n_features']}")
        print(f"   æœ‰æ•ˆç»´åº¦(90%): {features['dimension']['effective_dim_90']}")
        print(f"   å†…åœ¨ç»´åº¦: {features['dimension']['intrinsic_dim_estimate']:.1f}")
        print(f"   è¾¹ç•Œç‚¹æ¯”ä¾‹: {features['boundary']['boundary_ratio']:.2%}")
        print(f"   æ•°æ®æ¤­çƒæ€§: {features['shape']['eccentricity']:.2f}")
    
    def _print_final_results(self, result: Dict[str, Any]):
        """æ‰“å°æœ€ç»ˆç»“æœ"""
        print(f"\nğŸ‰ åˆ†æå®Œæˆ!")
        print("=" * 60)
        print(f"ğŸ“ˆ æœ€ç»ˆæ€§èƒ½:")
        perf = result['final_performance']
        print(f"   NMI: {perf['nmi']:.4f}")
        print(f"   ARI: {perf['ari']:.4f}")
        print(f"   æ€»ä½“æ”¹è¿›: {result['total_improvement']:+.4f}")
        print(f"   è¿­ä»£æ¬¡æ•°: {result['iterations_used']}")
        print(f"   å¤„ç†æ—¶é—´: {result['processing_time']:.1f}ç§’")
        
        # å­¦ä¹ è½¨è¿¹æ¦‚è¦
        trajectory = result['learning_trajectory']
        print(f"\nğŸ“Š å­¦ä¹ è½¨è¿¹:")
        for i, step in enumerate(trajectory):
            print(f"   ç¬¬{i+1}æ¬¡: NMI={step['nmi']:.4f}, ç­–ç•¥={step['strategy_name']}")
    
    def _save_results(self, result: Dict[str, Any], dataset_name: str):
        """ä¿å­˜ç»“æœ - å®‰å…¨ç‰ˆæœ¬"""
        output_dir = Path(self.config.output_dir) / dataset_name
        output_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            # åˆ›å»ºå®‰å…¨çš„ç»“æœå­—å…¸
            safe_result = {
                'dataset_name': str(result.get('dataset_name', 'unknown')),
                'data_shape': [int(x) for x in result.get('data_shape', [0, 0])],
                'final_performance': {
                    'nmi': float(result.get('final_performance', {}).get('nmi', 0)),
                    'ari': float(result.get('final_performance', {}).get('ari', 0))
                },
                'total_improvement': float(result.get('total_improvement', 0)),
                'iterations_used': int(result.get('iterations_used', 0)),
                'processing_time': float(result.get('processing_time', 0))
            }
            
            # ä¿å­˜ç®€åŒ–ç»“æœ
            import json
            with open(output_dir / 'analysis_result.json', 'w') as f:
                json.dump(safe_result, f, indent=2)
            
            print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
            
        except Exception as e:
            print(f"âš ï¸ JSONä¿å­˜å¤±è´¥ï¼Œä½¿ç”¨æ–‡æœ¬æ ¼å¼: {e}")
            # å¤‡ç”¨ï¼šä¿å­˜ä¸ºæ–‡æœ¬
            with open(output_dir / 'analysis_result.txt', 'w') as f:
                f.write(f"Dataset: {result.get('dataset_name', 'unknown')}\n")
                f.write(f"Data Shape: {result.get('data_shape', [0, 0])}\n")
                perf = result.get('final_performance', {})
                f.write(f"NMI: {perf.get('nmi', 0):.4f}\n")
                f.write(f"ARI: {perf.get('ari', 0):.4f}\n")
                f.write(f"Total Improvement: {result.get('total_improvement', 0):.4f}\n")
                f.write(f"Iterations: {result.get('iterations_used', 0)}\n")
                f.write(f"Time: {result.get('processing_time', 0):.1f}s\n")
            print(f"ğŸ’¾ å¤‡ç”¨æ–‡æœ¬ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
    
    def _update_atlas_knowledge(self, result: Dict[str, Any]):
        """æ›´æ–°å›¾è°±çŸ¥è¯†åº“"""
        self.strategy_atlas.update_knowledge(result)
        print("ğŸ§  å›¾è°±çŸ¥è¯†åº“å·²æ›´æ–°")
    
    def _make_json_serializable(self, obj):
        """å¢å¼ºçš„JSONåºåˆ—åŒ–å¤„ç†"""
        if isinstance(obj, (np.integer, np.int32, np.int64)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float32, np.float64)):
            return float(obj)
        elif isinstance(obj, (np.bool_)):
            return bool(obj)
        elif isinstance(obj, (np.ndarray)):
            return obj.tolist()
        elif isinstance(obj, (np.generic)):
            return obj.item()
        elif isinstance(obj, (pd.Timestamp, pd.DataFrame, pd.Series)):
            return str(obj)
        elif isinstance(obj, (tuple)):
            return list(obj)
        elif isinstance(obj, (dict)):
            return {self._make_json_serializable(k): self._make_json_serializable(v) 
                    for k, v in obj.items()}
        elif isinstance(obj, (list)):
            return [self._make_json_serializable(item) for item in obj]
        elif hasattr(obj, '__dict__'):
            return self._make_json_serializable(obj.__dict__)
        else:
            try:
                return str(obj)
            except:
                print(f"[è­¦å‘Š] æ— æ³•åºåˆ—åŒ–ç±»å‹: {type(obj)}")
                return None

    
    def _generate_batch_report(self, results: Dict, stats: Dict, output_dir: str):
        """ç”Ÿæˆæ‰¹é‡åˆ†ææŠ¥å‘Š"""
        # TODO: å®ç°è¯¦ç»†çš„æ‰¹é‡æŠ¥å‘Šç”Ÿæˆ
        pass

def main():
    """ä¸»å‡½æ•°"""
    import time
    start_time = time.time()
    parser = argparse.ArgumentParser(
        description="æ™ºèƒ½å•ç»†èƒRNAæµ‹åºåˆ†æç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åˆ†æå•ä¸ªæ•°æ®é›†
  python main.py --data data/usoskin.csv --labels data/usoskin_labels.csv --name Usoskin
  
  # æ‰¹é‡åˆ†æ
  python main.py --batch datasets_config.json
  
  # åŸºå‡†æµ‹è¯•
  python main.py --benchmark datasets_config.json --baselines SIMLR,scDSC
        """
    )
    
    # å•æ•°æ®é›†åˆ†æå‚æ•°
    parser.add_argument('--data', type=str, help='æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--labels', type=str, help='æ ‡ç­¾æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('--name', type=str, default='unknown', help='æ•°æ®é›†åç§°')
    parser.add_argument('--max-iter', type=int, default=5, help='æœ€å¤§è¿­ä»£æ¬¡æ•°')
    
    # æ‰¹é‡åˆ†æå‚æ•°
    parser.add_argument('--batch', type=str, help='æ‰¹é‡åˆ†æé…ç½®æ–‡ä»¶')
    
    # åŸºå‡†æµ‹è¯•å‚æ•°
    parser.add_argument('--benchmark', type=str, help='åŸºå‡†æµ‹è¯•é…ç½®æ–‡ä»¶')
    parser.add_argument('--baselines', type=str, help='åŸºçº¿æ–¹æ³•åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰')
    
    # ç³»ç»Ÿé…ç½®å‚æ•°
    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, default='results', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--verbose', action='store_true', help='è¯¦ç»†è¾“å‡ºæ¨¡å¼')
    
    args = parser.parse_args()
    
    # åˆ›å»ºç³»ç»Ÿå®ä¾‹
    try:
        system = IntelligentScRNAAtlas(args.config)
        
        # æ ¹æ®å‚æ•°é€‰æ‹©è¿è¡Œæ¨¡å¼
        if args.batch:
            # æ‰¹é‡åˆ†ææ¨¡å¼
            results = system.batch_analyze(args.batch, args.output)
            
        elif args.benchmark:
            # åŸºå‡†æµ‹è¯•æ¨¡å¼
            baselines = args.baselines.split(',') if args.baselines else None
            results = system.benchmark_against_baselines(args.benchmark, baselines)
            
        elif args.data:
            # å•æ•°æ®é›†åˆ†ææ¨¡å¼
            result = system.analyze_dataset(
                args.data, 
                args.labels, 
                args.name,
                args.max_iter
            )
            
        else:
            parser.print_help()
            return
        
        print(f"\nâœ… ç³»ç»Ÿè¿è¡Œå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ ç³»ç»Ÿè¿è¡Œé”™è¯¯: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1

    total_time = time.time() - start_time
    print(f"\nğŸ•’ ç¨‹åºæ€»è¿è¡Œæ—¶é—´: {total_time:.2f} ç§’")

    return 0

if __name__ == "__main__":
    exit(main())
