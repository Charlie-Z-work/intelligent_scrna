#!/usr/bin/env python3
"""
ğŸ¯ éšæœºçŠ¶æ€è®°å½•å®éªŒ - ä¿®å¤ç‰ˆ (æ— warnings)
è®°å½•æ‰€æœ‰éšæœºæ€§æ¥æºï¼Œç”¨äºåç»­å®Œå…¨å¤ç°
"""

import sys
import os
sys.path.append('..')

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.parameter import Parameter
from torch.optim import Adam
from torch.nn import Linear
from utils import load_data, load_graph
from evaluation import eva
import h5py
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score
from layers import ZINBLoss, MeanAct, DispAct
from GNN import GNNLayer
import time
import random
import json
import pickle
from datetime import datetime
import warnings

# æŠ‘åˆ¶FutureWarning
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=UserWarning)

class RandomStateManager:
    """éšæœºçŠ¶æ€ç®¡ç†å™¨"""
    
    def __init__(self):
        self.states = {}
        
    def capture_all_states(self, label=""):
        """æ•è·æ‰€æœ‰éšæœºçŠ¶æ€"""
        states = {
            'label': label,
            'timestamp': datetime.now().isoformat(),
            'python_random_state': random.getstate(),
            'numpy_random_state': np.random.get_state(),
            'torch_random_state': torch.get_rng_state(),
            'torch_cuda_random_state': torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,
        }
        return states
    
    def restore_all_states(self, states):
        """æ¢å¤æ‰€æœ‰éšæœºçŠ¶æ€"""
        random.setstate(states['python_random_state'])
        np.random.set_state(states['numpy_random_state'])
        torch.set_rng_state(states['torch_random_state'])
        if torch.cuda.is_available() and states['torch_cuda_random_state'] is not None:
            torch.cuda.set_rng_state_all(states['torch_cuda_random_state'])
    
    def save_states(self, states, filename):
        """ä¿å­˜çŠ¶æ€åˆ°æ–‡ä»¶"""
        with open(filename, 'wb') as f:
            pickle.dump(states, f)
    
    def load_states(self, filename):
        """ä»æ–‡ä»¶åŠ è½½çŠ¶æ€"""
        with open(filename, 'rb') as f:
            return pickle.load(f)

def set_initial_seed(seed):
    """è®¾ç½®åˆå§‹ç§å­"""
    print(f"ğŸ”’ è®¾ç½®åˆå§‹ç§å­: {seed}")
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)

class AE(nn.Module):
    def __init__(self, n_enc_1, n_enc_2, n_enc_3, n_dec_1, n_dec_2, n_dec_3, n_input, n_z):
        super(AE, self).__init__()
        self.enc_1 = Linear(n_input, n_enc_1)
        self.BN1 = nn.BatchNorm1d(n_enc_1)
        self.enc_2 = Linear(n_enc_1, n_enc_2)
        self.BN2 = nn.BatchNorm1d(n_enc_2)
        self.enc_3 = Linear(n_enc_2, n_enc_3)
        self.BN3 = nn.BatchNorm1d(n_enc_3)
        self.z_layer = Linear(n_enc_3, n_z)
        self.dec_1 = Linear(n_z, n_dec_1)
        self.BN4 = nn.BatchNorm1d(n_dec_1)
        self.dec_2 = Linear(n_dec_1, n_dec_2)
        self.BN5 = nn.BatchNorm1d(n_dec_2)
        self.dec_3 = Linear(n_dec_2, n_dec_3)
        self.BN6 = nn.BatchNorm1d(n_dec_3)
        self.x_bar_layer = Linear(n_dec_3, n_input)

    def forward(self, x):
        enc_h1 = F.relu(self.BN1(self.enc_1(x)))
        enc_h2 = F.relu(self.BN2(self.enc_2(enc_h1)))
        enc_h3 = F.relu(self.BN3(self.enc_3(enc_h2)))
        z = self.z_layer(enc_h3)
        dec_h1 = F.relu(self.BN4(self.dec_1(z)))
        dec_h2 = F.relu(self.BN5(self.dec_2(dec_h1)))
        dec_h3 = F.relu(self.BN6(self.dec_3(dec_h2)))
        x_bar = self.x_bar_layer(dec_h3)
        return x_bar, enc_h1, enc_h2, enc_h3, z, dec_h3

class SDCN_StateTracked(nn.Module):
    def __init__(self, n_enc_1, n_enc_2, n_enc_3, n_dec_1, n_dec_2, n_dec_3,
                 n_input, n_z, n_clusters, v=1, use_zinb=True, pretrain_path='model/mtab.pkl'):
        super(SDCN_StateTracked, self).__init__()
        self.use_zinb = use_zinb
        
        self.ae = AE(n_enc_1, n_enc_2, n_enc_3, n_dec_1, n_dec_2, n_dec_3, n_input, n_z)
        
        if os.path.exists(pretrain_path):
            # ä¿®å¤ï¼šæ·»åŠ weights_onlyå‚æ•°
            self.ae.load_state_dict(torch.load(pretrain_path, map_location='cpu', weights_only=False))

        self.gnn_1 = GNNLayer(n_input, n_enc_1)
        self.gnn_2 = GNNLayer(n_enc_1, n_enc_2)
        self.gnn_3 = GNNLayer(n_enc_2, n_enc_3)
        self.gnn_4 = GNNLayer(n_enc_3, n_z)
        self.gnn_5 = GNNLayer(n_z, n_clusters)
        
        self.cluster_layer = Parameter(torch.Tensor(n_clusters, n_z))
        torch.nn.init.xavier_normal_(self.cluster_layer.data, gain=1.0)

        if self.use_zinb:
            self._dec_mean = nn.Sequential(nn.Linear(n_dec_3, n_input), MeanAct())
            self._dec_disp = nn.Sequential(nn.Linear(n_dec_3, n_input), DispAct())
            self._dec_pi = nn.Sequential(nn.Linear(n_dec_3, n_input), nn.Sigmoid())
            self.zinb_loss = ZINBLoss()

        self.v = v

    def forward(self, x, adj, sigma=0.5):
        x_bar, tra1, tra2, tra3, z, dec_h3 = self.ae(x)
        h = self.gnn_1(x, adj)
        h = self.gnn_2((1 - sigma) * h + sigma * tra1, adj)
        h = self.gnn_3((1 - sigma) * h + sigma * tra2, adj)
        h = self.gnn_4((1 - sigma) * h + sigma * tra3, adj)
        h = self.gnn_5((1 - sigma) * h + sigma * z, adj, active=False)
        predict = F.softmax(h, dim=1)

        if self.use_zinb:
            _mean = self._dec_mean(dec_h3)
            _disp = self._dec_disp(dec_h3)
            _pi = self._dec_pi(dec_h3)
        else:
            _mean = _disp = _pi = None

        q = 1.0 / (1.0 + torch.sum(torch.pow(z.unsqueeze(1) - self.cluster_layer, 2), 2) / self.v)
        q = q.pow((self.v + 1.0) / 2.0)
        q = (q.t() / torch.sum(q, 1)).t()
        return x_bar, q, predict, z, _mean, _disp, _pi

def target_distribution(q):
    weight = q ** 2 / q.sum(0)
    return (weight.t() / weight.sum(1)).t()

def run_single_experiment(experiment_id, total_experiments, config, epochs=80):
    """è¿è¡Œå•æ¬¡å®éªŒå¹¶è®°å½•æ‰€æœ‰çŠ¶æ€"""
    
    print(f"\nğŸ§ª å®éªŒ {experiment_id}/{total_experiments}")
    
    state_mgr = RandomStateManager()
    initial_states = state_mgr.capture_all_states("experiment_start")
    start_time = time.time()
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # åŠ è½½æ•°æ®
    with h5py.File("data/mtab.h5", "r") as f:
        X_raw = np.array(f['X_raw'])
        X_scaled = np.array(f['X'])
        y = np.array(f['Y'])
        size_factors = np.array(f['size_factors'])
    
    post_data_states = state_mgr.capture_all_states("post_data_loading")
    
    # åˆ›å»ºæ¨¡å‹
    model = SDCN_StateTracked(500, 500, 2000, 2000, 500, 500, 5000, 20, 8, v=1, use_zinb=True).to(device)
    post_model_states = state_mgr.capture_all_states("post_model_creation")
    
    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=1e-5)
    
    # åŠ è½½å›¾å’Œæ•°æ®
    adj = load_graph('mtab_processed', None).to(device)
    data = torch.Tensor(X_scaled).to(device)
    X_raw_tensor = torch.tensor(X_raw, dtype=torch.float32).to(device)
    sf_tensor = torch.tensor(size_factors, dtype=torch.float32).to(device)
    
    # K-meansåˆå§‹åŒ–
    with torch.no_grad():
        _, _, _, _, z, _ = model.ae(data)
    
    pre_kmeans_states = state_mgr.capture_all_states("pre_kmeans")
    
    kmeans = KMeans(n_clusters=8, n_init=20, random_state=42, max_iter=300)
    y_pred = kmeans.fit_predict(z.data.cpu().numpy())
    model.cluster_layer.data = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32).to(device)
    
    post_kmeans_states = state_mgr.capture_all_states("post_kmeans")
    
    # è®­ç»ƒè®°å½•
    best_ari = 0
    best_nmi = 0
    best_epoch = 0
    
    global p
    for epoch in range(epochs):
        if epoch % 5 == 0:
            with torch.no_grad():
                _, tmp_q, pred, _, _, _, _ = model(data, adj, sigma=config['sigma'])
                p = target_distribution(tmp_q.data)
                
                res2 = pred.data.cpu().numpy().argmax(1)
                ari_z = adjusted_rand_score(y, res2)
                nmi_z = normalized_mutual_info_score(y, res2)
                
                if ari_z > best_ari:
                    best_ari = ari_z
                    best_nmi = nmi_z
                    best_epoch = epoch
        
        # å‰å‘ä¼ æ’­
        x_bar, q, pred, z, meanbatch, dispbatch, pibatch = model(data, adj, sigma=config['sigma'])
        
        # æŸå¤±è®¡ç®—
        kl_loss = F.kl_div(q.log(), p, reduction='batchmean')
        ce_loss = F.kl_div(pred.log(), p, reduction='batchmean')
        re_loss = F.mse_loss(x_bar, data)
        
        try:
            zinb_loss_value = model.zinb_loss(X_raw_tensor, meanbatch, dispbatch, pibatch, sf_tensor)
            if torch.isnan(zinb_loss_value) or torch.isinf(zinb_loss_value):
                zinb_loss_value = torch.tensor(0.0, device=device)
        except:
            zinb_loss_value = torch.tensor(0.0, device=device)
        
        total_loss = (config['alpha'] * kl_loss + config['beta'] * ce_loss + 
                     config['gamma'] * re_loss + config['delta'] * zinb_loss_value)
        
        optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
    
    end_time = time.time()
    training_time = end_time - start_time
    
    final_states = state_mgr.capture_all_states("experiment_end")
    
    experiment_result = {
        'experiment_id': experiment_id,
        'config': config,
        'performance': {
            'ari': best_ari,
            'nmi': best_nmi,
            'best_epoch': best_epoch,
            'score': 0.6 * best_ari + 0.4 * best_nmi
        },
        'timing': {
            'total_time_seconds': training_time,
            'time_per_epoch': training_time / epochs,
            'epochs': epochs
        },
        'random_states': {
            'initial': initial_states,
            'post_data_loading': post_data_states,
            'post_model_creation': post_model_states,
            'pre_kmeans': pre_kmeans_states,
            'post_kmeans': post_kmeans_states,
            'final': final_states
        }
    }
    
    print(f"  ğŸ† ARI={best_ari:.4f}, NMI={best_nmi:.4f}, Epoch={best_epoch}")
    print(f"  â±ï¸  è®­ç»ƒæ—¶é—´: {training_time:.1f}ç§’ ({training_time/epochs:.2f}ç§’/è½®)")
    
    return experiment_result

def run_random_state_discovery(num_experiments=50):
    """è¿è¡ŒéšæœºçŠ¶æ€å‘ç°å®éªŒ"""
    
    print("ğŸ¯ å¤§è§„æ¨¡éšæœºçŠ¶æ€å‘ç°å®éªŒ")
    print("="*60)
    print(f"ğŸ“Š å®éªŒæ¬¡æ•°: {num_experiments}")
    print(f"ğŸ° ç­–ç•¥: é™¤KMeanså¤–å…¨éƒ¨éšæœº")
    print(f"ğŸ¯ ç›®æ ‡: å¯»æ‰¾èƒ½è¾¾åˆ°ARI=0.77+çš„ç§å­")
    
    # ä¼ è¯´é…ç½®
    config = {
        'alpha': 0.04, 'beta': 0.005, 'gamma': 0.85, 'delta': 0.18,
        'lr': 1.2e-4, 'sigma': 0.55
    }
    
    all_results = []
    total_start_time = time.time()
    
    for i in range(1, num_experiments + 1):
        # ä¸ºæ¯ä¸ªå®éªŒè®¾ç½®ä¸åŒçš„åˆå§‹ç§å­
        initial_seed = random.randint(1, 100000)
        set_initial_seed(initial_seed)
        
        result = run_single_experiment(i, num_experiments, config, epochs=80)
        result['initial_seed'] = initial_seed
        all_results.append(result)
        
        # å®æ—¶æ˜¾ç¤ºè¿›åº¦
        if i % 10 == 0:
            current_best = max(all_results, key=lambda x: x['performance']['score'])
            print(f"\nğŸ“Š è¿›åº¦ {i}/{num_experiments} - å½“å‰æœ€ä½³: ARI={current_best['performance']['ari']:.4f}")
    
    total_time = time.time() - total_start_time
    
    # æ‰¾åˆ°æœ€ä½³ç»“æœ
    best_result = max(all_results, key=lambda x: x['performance']['score'])
    
    # æ‰¾åˆ°æ‰€æœ‰è¶…è¿‡0.7çš„ç»“æœ
    high_performers = [r for r in all_results if r['performance']['ari'] > 0.7]
    
    print(f"\nğŸ† å¤§è§„æ¨¡å‘ç°å®éªŒæ€»ç»“:")
    print("="*60)
    print(f"ğŸ• æ€»è¿è¡Œæ—¶é—´: {total_time:.1f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
    print(f"â±ï¸  å¹³å‡å•æ¬¡æ—¶é—´: {total_time/num_experiments:.1f}ç§’")
    print(f"ğŸ“Š å¹³å‡æ¯è½®æ—¶é—´: {np.mean([r['timing']['time_per_epoch'] for r in all_results]):.2f}ç§’")
    
    print(f"\nğŸ¥‡ æœ€ä½³ç»“æœ:")
    print(f"  ğŸ†” å®éªŒID: {best_result['experiment_id']}")
    print(f"  ğŸ² åˆå§‹ç§å­: {best_result['initial_seed']}")
    print(f"  ğŸ† ARI: {best_result['performance']['ari']:.4f}")
    print(f"  ğŸ¯ NMI: {best_result['performance']['nmi']:.4f}")
    print(f"  ğŸ“Š Score: {best_result['performance']['score']:.4f}")
    print(f"  ğŸ æœ€ä½³è½®æ¬¡: {best_result['performance']['best_epoch']}")
    
    if high_performers:
        print(f"\nğŸ¯ è¶…è¿‡ARI=0.7çš„ç§å­ ({len(high_performers)}ä¸ª):")
        for hp in sorted(high_performers, key=lambda x: x['performance']['ari'], reverse=True)[:5]:
            print(f"  ç§å­{hp['initial_seed']}: ARI={hp['performance']['ari']:.4f}, NMI={hp['performance']['nmi']:.4f}")
    else:
        print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        aris = [r['performance']['ari'] for r in all_results]
        print(f"  ARIèŒƒå›´: {min(aris):.4f} - {max(aris):.4f}")
        print(f"  ARIå¹³å‡: {np.mean(aris):.4f} Â± {np.std(aris):.4f}")
    
    # ä¿å­˜æ‰€æœ‰ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"results/logs/large_scale_discovery_{timestamp}.pkl"
    
    discovery_summary = {
        'experiment_info': {
            'num_experiments': num_experiments,
            'total_time_seconds': total_time,
            'config': config,
            'timestamp': timestamp
        },
        'all_results': all_results,
        'best_result': best_result,
        'high_performers': high_performers,
        'performance_stats': {
            'ari_mean': np.mean([r['performance']['ari'] for r in all_results]),
            'ari_std': np.std([r['performance']['ari'] for r in all_results]),
            'ari_max': max([r['performance']['ari'] for r in all_results]),
            'ari_min': min([r['performance']['ari'] for r in all_results])
        }
    }
    
    with open(results_file, 'wb') as f:
        pickle.dump(discovery_summary, f)
    
    print(f"\nğŸ“ å®Œæ•´ç»“æœä¿å­˜è‡³: {results_file}")
    
    # ä¿å­˜æœ€ä½³ç»“æœçš„çŠ¶æ€æ–‡ä»¶
    best_states_file = f"results/logs/best_states_50exp_{timestamp}.pkl"
    with open(best_states_file, 'wb') as f:
        pickle.dump(best_result['random_states'], f)
    
    print(f"ğŸ¯ æœ€ä½³çŠ¶æ€ä¿å­˜è‡³: {best_states_file}")
    
    return discovery_summary

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--num_experiments', type=int, default=50, help='å®éªŒæ¬¡æ•°')
    args = parser.parse_args()
    
    os.makedirs("results/logs", exist_ok=True)
    
    print(f"ğŸš€ å¼€å§‹å¤§è§„æ¨¡éšæœºçŠ¶æ€å‘ç°å®éªŒ...")
    discovery_summary = run_random_state_discovery(args.num_experiments)
